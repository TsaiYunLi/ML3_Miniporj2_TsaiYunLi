# ML3_Miniporj2_TsaiYunLi
This is a NLP binary classification practice task on the Kaggle platform. The goal of this Kaggle task is to detect tweets about disasters and label them as 1 when positive and 0 when negative.

I have decided to build and compare an Long Short-Term Memory (LSTM) model and a Gated Recurrent (GRU) model for this Kaggle task. These two recurrent neural network (RNN) models are suitable for this task because that they can capture long-term dependency and have context sensitivity. The tweets can be viewed as sequences of words, in which the meaning of each words can depend on previous words. The LSTM and GRU models can handle this kind of sequential data.

See links in the reference for the Kaggle competition overview and datasets. 
